# Clothing Segmentation Pipeline

## Overview
This project implements a U-Net based semantic segmentation model to automatically detect and extract clothing items from images. It uses **PyTorch**, **Segmentation Models PyTorch (SMP)**, and **Albumentations** for robust data augmentation.

The model is designed to produce high-quality binary masks (Clothing vs. Background), enabling "Virtual Try-On" applications by creating transparent cutouts of clothing.

### Run on Google Colab

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Amr789/clothing-segmentation/)


## Key Features
* **Architecture:** U-Net with MobileNetV2 backbone (Pre-trained on ImageNet).
* **Data Pipeline:** Efficient multi-threaded downloading and processing of the ATR dataset.
* **Loss Function:** Hybrid Loss (Dice Loss + Binary Cross Entropy) for balanced optimization.
* **Inference:** Production-ready inference class that handles resizing, padding, and RGBA cutout generation.

## Project Structure

```text
clothing-segmentation/
├── data/                    # Dataset storage (Generated by setup)
├── src/                     # Source code package
│   ├── __init__.py
│   ├── config.py            # Central configuration settings
│   ├── data_setup.py        # Data downloading & extraction scripts
│   ├── dataset.py           # PyTorch Dataset class & Albumentations
│   ├── engine.py            # Training and Evaluation loops
│   ├── inference.py         # Production inference pipeline
│   ├── loss.py              # Custom Hybrid Loss (Dice + BCE)
│   ├── model.py             # U-Net Model architecture
│   └── utils.py             # Visualization & Seeding utilities
├── .gitignore               # Files to exclude from Git
├── LICENSE                  # MIT License
├── main.py                  # CLI Entry point (Train/Predict)
├── README.md                # Project documentation
├── Test.jpg                # Image that can be used to test the model
└── requirements.txt         # Python dependencies
```


## Installation

### Prerequisites
* Python 3.8+
* GPU recommended (CUDA)

## Run on Google Colab

You can run this project entirely in the cloud using Google Colab's free GPUs (T4).

